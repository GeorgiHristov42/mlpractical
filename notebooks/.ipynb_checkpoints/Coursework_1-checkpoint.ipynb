{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer, ELULayer, SELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit, SELUInit, UniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model with Sigmoid units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b02e090a7fa40329b1ac0e7ddb6342b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a6c309c84e402d9afa74a23988fec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1.2s to complete\n",
      "    error(train)=1.06e+00, acc(train)=7.48e-01, error(valid)=1.04e+00, acc(valid)=7.74e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0eb659b1cf462cb56c28f56411a5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-7180f538da37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Remember to use notebook=False when you write a script to be run in a terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n\u001b[0;32m---> 23\u001b[0;31m     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    final error(train) = {0:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error(train)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-6b96ccc6ce81>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mdo_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mtrain_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch Progress\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mgrads_wrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 grads_wrt_params = self.model.grads_wrt_params(\n",
      "\u001b[0;32m~/mlpractical/mlp/models.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs, stochastic)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     activations[i], stochastic=stochastic))\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/layers.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('SigmoidFig1.pdf')\n",
    "fig_2.savefig('SigmoidFig2.pdf')\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model with ReLu units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('ReluFig1.pdf')\n",
    "fig_2.savefig('ReluFig2.pdf')\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('LeakyReluFig1.pdf')\n",
    "fig_2.savefig('LeakyReluFig2.pdf')\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELULayer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('Elu1.pdf')\n",
    "fig_2.savefig('Elu2.pdf')\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELULayer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 5\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    SELULayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('Selu1.pdf')\n",
    "fig_2.savefig('Selu2.pdf')\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ActivationLayers   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |\n",
      "|--------------------|--------------------|--------------------|------------------|------------------|\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-08abf574fcb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     print('| {0:.3f}           | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n\u001b[1;32m      7\u001b[0m           .format(ActivationLayers, \n\u001b[0;32m----> 8\u001b[0;31m                   \u001b[0mfinal_errors_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_errors_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                   final_accs_train[j], final_accs_valid[j]))\n\u001b[1;32m     10\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "ActivationLayers = ['Sigmoid', 'Relu','LeakyRelu', 'ELU', 'SELU']\n",
    "print('| ActivationLayers   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|--------------------|--------------------|--------------------|------------------|------------------|')\n",
    "for ActivationLayers in ActivationLayers:\n",
    "    print('| {0:.3f}           | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n",
    "          .format(ActivationLayers, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_final_errors_train = []\n",
    "lr_final_errors_valid = []\n",
    "lr_final_accs_train = []\n",
    "lr_final_accs_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('3Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('3Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('4Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('4Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('5Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('5Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 6 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('6Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('6Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 7 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('7Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('7Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu model with 8 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2aba912d1b4ee598be159de6221141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f76a5deb97441c8dc1e564780ded20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-2a6f0ebf801b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#Remember to use notebook=False when you write a script to be run in a terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n\u001b[0;32m---> 36\u001b[0;31m     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    final error(train) = {0:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error(train)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-6b96ccc6ce81>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mdo_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mgrads_wrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 grads_wrt_params = self.model.grads_wrt_params(\n\u001b[0;32m---> 70\u001b[0;31m                     activations, grads_wrt_outputs)\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_wrt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mtrain_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/models.py\u001b[0m in \u001b[0;36mgrads_wrt_params\u001b[0;34m(self, activations, grads_wrt_outputs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# give gradients in consistent order with self.params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 grads_wrt_params += layer.grads_wrt_params(\n\u001b[0;32m--> 130\u001b[0;31m                     inputs, grads_wrt_outputs)[::-1]\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;31m# If not at first layer back-propagate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/layers.py\u001b[0m in \u001b[0;36mgrads_wrt_params\u001b[0;34m(self, inputs, grads_wrt_outputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mgrads_wrt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mgrads_wrt_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('8Layer_LeakyRelu1.pdf')\n",
    "fig_2.savefig('8Layer_LeakyRelu2.pdf')\n",
    "\n",
    "lr_final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "lr_final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "lr_final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "lr_final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| layers   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |\n",
      "|-----------------|--------------------|--------------------|------------------|------------------|\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_final_errors_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-1a8d0ce69122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     print('| {0:.3f}           | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n\u001b[1;32m      7\u001b[0m           .format(layers, \n\u001b[0;32m----> 8\u001b[0;31m                   \u001b[0mlr_final_errors_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_final_errors_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                   lr_final_accs_train[j], lr_final_accs_valid[j]))\n\u001b[1;32m     10\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_final_errors_train' is not defined"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "layers = range(3,8)\n",
    "print('| layers   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|-----------------|--------------------|--------------------|------------------|------------------|')\n",
    "for layers in layers:\n",
    "    print('| {0:.3f}           | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n",
    "          .format(layers, \n",
    "                  lr_final_errors_train[j], lr_final_errors_valid[j],\n",
    "                  lr_final_accs_train[j], lr_final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| learning_rate   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |\n",
    "|-----------------|--------------------|--------------------|------------------|------------------|\n",
    "| 0.100           | 1.23e-05           | 1.55e-01           |  1.00            | 0.98             |\n",
    "| 0.050           | 3.94e-05           | 1.66e-01           |  1.00            | 0.98             |\n",
    "| 0.010           | 8.57e-04           | 1.62e-01           |  1.00            | 0.97             |\n",
    "| 0.001           | 1.33e-01           | 1.44e-01           |  0.96            | 0.96             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the effect of different initialisation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fan-in: w i  U( (3/n in) , (3/n in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853da4fb676d4a368abec59d87951baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad0010747d4499e91bdc0d5b7922f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1.1s to complete\n",
      "    error(train)=2.05e+00, acc(train)=2.91e-01, error(valid)=2.05e+00, acc(valid)=2.86e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f1a03527c74923861eb206ebe08d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 1.6s to complete\n",
      "    error(train)=4.01e-01, acc(train)=8.86e-01, error(valid)=3.77e-01, acc(valid)=8.93e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db76fb3770464410bef682405d58b64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 1.4s to complete\n",
      "    error(train)=2.19e-01, acc(train)=9.40e-01, error(valid)=2.14e-01, acc(valid)=9.39e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bc5d1434c84609aa446ca8ff5b4508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1.3s to complete\n",
      "    error(train)=1.62e-01, acc(train)=9.53e-01, error(valid)=1.75e-01, acc(valid)=9.48e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4881e935427f43e5837be8d557141c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 1.4s to complete\n",
      "    error(train)=1.11e-01, acc(train)=9.68e-01, error(valid)=1.34e-01, acc(valid)=9.62e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec2fab831414d2d967fc78237a7b088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 1.3s to complete\n",
      "    error(train)=8.21e-02, acc(train)=9.76e-01, error(valid)=1.15e-01, acc(valid)=9.68e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d3abf5e4de4192abcbf061105dee0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 1.2s to complete\n",
      "    error(train)=7.11e-02, acc(train)=9.79e-01, error(valid)=1.09e-01, acc(valid)=9.68e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f722c6c03464b468a58cbffa3b57731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 1.2s to complete\n",
      "    error(train)=6.27e-02, acc(train)=9.82e-01, error(valid)=1.11e-01, acc(valid)=9.69e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d906ee0944b34a3b8005237b2dbf6a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 1.3s to complete\n",
      "    error(train)=5.10e-02, acc(train)=9.85e-01, error(valid)=1.02e-01, acc(valid)=9.72e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb6d21859904898a6a6d645c10d88ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 1.2s to complete\n",
      "    error(train)=4.51e-02, acc(train)=9.86e-01, error(valid)=1.07e-01, acc(valid)=9.72e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ed516de95c41e783395a949b133280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 1.5s to complete\n",
      "    error(train)=3.53e-02, acc(train)=9.90e-01, error(valid)=1.06e-01, acc(valid)=9.71e-01, params_penalty=0.00e+00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2740aac1964ef1957ee66ed131401d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-d3e08f4db325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#Remember to use notebook=False when you write a script to be run in a terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n\u001b[0;32m---> 32\u001b[0;31m     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    final error(train) = {0:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error(train)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-6b96ccc6ce81>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epoch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0mrun_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36mget_epoch_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[1;32m    103\u001b[0m         \u001b[0mepoch_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mepoch_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_monitors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             epoch_stats.update(self.eval_monitors(\n",
      "\u001b[0;32m~/mlpractical/mlp/optimisers.py\u001b[0m in \u001b[0;36meval_monitors\u001b[0;34m(self, dataset, label)\u001b[0m\n\u001b[1;32m     85\u001b[0m         data_mon_vals = OrderedDict([(key + label, 0.) for key\n\u001b[1;32m     86\u001b[0m                                      in self.data_monitors.keys()])\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_monitor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_monitors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/data_providers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/data_providers.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;34m\"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_one_of_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/data_providers.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# no more batches in current iteration through data set so start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# new epoch ready for another pass and indicate iteration is at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# create an index slice corresponding to current batch number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/data_providers.py\u001b[0m in \u001b[0;36mnew_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlpractical/mlp/data_providers.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "\n",
    "# Create a parameter initialiser which will sample random uniform values\n",
    "# from [-init_scale, init_scale]\n",
    "init_scale = math.sqrt(3/input_dim)\n",
    "param_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, param_init, param_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('4Layer_LeakyRelu1_FanIn.pdf')\n",
    "fig_2.savefig('4Layer_LeakyRelu2_FanIn.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fan-out: wi  U( 3/n out , 3/n\n",
    " out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.02\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "\n",
    "# Create a parameter initialiser which will sample random uniform values\n",
    "# from [-init_scale, init_scale]\n",
    "init_scale = math.sqrt(3/output_dim)\n",
    "param_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, param_init, param_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('4Layer_LeakyRelu1_FanOut.pdf')\n",
    "fig_2.savefig('4Layer_LeakyRelu2_FanOut.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "\n",
    "# Create a parameter initialiser which will sample random uniform values\n",
    "# from [-init_scale, init_scale]\n",
    "init_scale = math.sqrt(6/(input_dim + output_dim))\n",
    "param_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, param_init, param_init), \n",
    "    LeakyReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, param_init, param_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "fig_1.savefig('4Layer_LeakyRelu1_FanInOut.pdf')\n",
    "fig_2.savefig('4Layer_LeakyRelu2_FanInOut.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "init_scale = [math.sqrt(3/input_dim), math.sqrt(3/output_dim), math.sqrt(6/input_dim + output_dim)]\n",
    "print('| init_scale   | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|--------------|--------------------|--------------------|------------------|------------------|')\n",
    "for init_scale in init_scale:\n",
    "    print('| {0:.3f}      | {1:.2e}           | {2:.2e}           |  {3:.2f}            | {4:.2f}             |'\n",
    "          .format(learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
